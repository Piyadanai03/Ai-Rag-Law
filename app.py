import os
import json
import faiss
import torch
import numpy as np
import requests
import io
import pdfplumber
from PIL import Image
import pytesseract
from typing import List
from sentence_transformers import SentenceTransformer
from flask import Flask, request, jsonify, render_template, session

# ===== CONFIG =====
OLLAMA_URL = os.getenv("OLLAMA_URL", "http://localhost:11434/api/generate")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL","llama3.2")

FOLDER_LAW = ["data", "family"]
FOLDER_LAWYER = ["lawyer"]

INDEX_LAW = "embeddings/faiss_law.index"
INDEX_LAWYER = "embeddings/faiss_lawyer.index"
TEXTS_LAW = "cache/law.json"
TEXTS_LAWYER = "cache/lawyer.json"

# ===== INIT =====
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"üñ•Ô∏è ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏ö‡∏ô: {device.upper()}")

app = Flask(__name__)
app.secret_key = "supersecretkey"

embedder = SentenceTransformer("intfloat/multilingual-e5-base")
embedder.to(device)

# ===== GLOBAL =====
law_texts, lawyer_texts = [], []
law_index, lawyer_index = None, None
lawyer_data = []


# ===== UTILITIES =====
def load_json_files(folder_paths: List[str]):
    texts = []
    for folder in folder_paths:
        if not os.path.exists(folder):
            continue
        for f in os.listdir(folder):
            if f.endswith(".json"):
                with open(os.path.join(folder, f), "r", encoding="utf-8") as fp:
                    data = json.load(fp)
                    if isinstance(data, list):
                        texts.extend(data)
                    else:
                        texts.append(data)
    return texts


def embed_texts(texts: List[str]):
    return embedder.encode(
        texts,
        batch_size=16,
        show_progress_bar=True,
        convert_to_numpy=True,
        normalize_embeddings=True,
        device=device
    )


def build_index(embeddings: np.ndarray):
    index = faiss.IndexFlatIP(embeddings.shape[1])
    index.add(embeddings)
    return index


def search_similar(query: str, texts: List[str], index, top_k=5):
    q_emb = embedder.encode([query], convert_to_numpy=True, normalize_embeddings=True, device=device)
    scores, ids = index.search(q_emb, top_k)
    return [(texts[i], float(scores[0][idx])) for idx, i in enumerate(ids[0])]


def extract_text_from_pdf(file_bytes):
    text = ""
    with pdfplumber.open(io.BytesIO(file_bytes)) as pdf:
        for page in pdf.pages:
            text += page.extract_text() or ""
    return text.strip()


def extract_text_from_image(file_bytes):
    img = Image.open(io.BytesIO(file_bytes))
    return pytesseract.image_to_string(img, lang="tha+eng").strip()


# ===== BUILD DATASET =====
def build_law_dataset():
    data = load_json_files(FOLDER_LAW)
    texts = [f"{d['law_name']} ‡∏°‡∏≤‡∏ï‡∏£‡∏≤ {d['section_num']}: {d['section_content']}"
             for d in data if "law_name" in d and "section_content" in d]
    emb = embed_texts(texts)
    return texts, build_index(emb)


def build_lawyer_dataset():
    data = load_json_files(FOLDER_LAWYER)
    texts = [f"{l['name']} - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç: {', '.join(l['expertise'])}"
             for l in data if "name" in l and "expertise" in l]
    emb = embed_texts(texts)
    return data, texts, build_index(emb)


# ===== ROUTES =====
@app.route("/")
def home():
    return render_template("index.html")


@app.route("/chat", methods=["POST"])
def chat():
    """
    ‡πÅ‡∏ä‡∏ó‡∏£‡∏ß‡∏°:
    - ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå ‚Üí ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏™‡∏∞‡∏Å‡∏î‡∏Å‡πà‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏™‡∏£‡∏∏‡∏õ ‚Üí ‡πÄ‡∏Å‡πá‡∏ö‡πÉ‡∏ô session
    - ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‚Üí ‡πÉ‡∏ä‡πâ context + RAG + AI ‡∏ï‡∏≠‡∏ö
    - ‡∏ñ‡πâ‡∏≤‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏° "‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏ô‡∏≤‡∏¢" ‚Üí ‡∏™‡πà‡∏á action='get_lawyer'
    """

    # ‚úÖ ‡∏ï‡∏£‡∏ß‡∏à‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô action ‡∏Ç‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏ô‡∏≤‡∏¢‡∏à‡∏≤‡∏Å‡∏õ‡∏∏‡πà‡∏°
    if request.is_json:
        data = request.get_json()
        if data.get("action") == "get_lawyer":
            lawyer = data.get("lawyer")
            return jsonify({
                "type": "lawyer",
                "message": "üìû ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏ô‡∏≤‡∏¢",
                "lawyer": lawyer
            })

    # ==== ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå ====
    if "file" in request.files:
        f = request.files["file"]
        name = f.filename.lower()
        fb = f.read()

        # ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å PDF / ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û
        if name.endswith(".pdf"):
            text = extract_text_from_pdf(fb)
        elif name.endswith((".png", ".jpg", ".jpeg")):
            text = extract_text_from_image(fb)
        else:
            return jsonify({"error": "‚ö†Ô∏è ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞ PDF ‡∏´‡∏£‡∏∑‡∏≠‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô"}), 400

        if not text:
            return jsonify({"error": "‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡πÑ‡∏ü‡∏•‡πå"}), 400

        # ===== ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 1: ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏™‡∏∞‡∏Å‡∏î / ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡πÉ‡∏´‡∏°‡πà =====
        prompt_correct = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
‡πÇ‡∏õ‡∏£‡∏î‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ:
- ‡∏™‡∏∞‡∏Å‡∏î‡∏Ñ‡∏≥‡πÅ‡∏•‡∏∞‡πÄ‡∏ß‡πâ‡∏ô‡∏ß‡∏£‡∏£‡∏Ñ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏´‡∏•‡∏±‡∏Å‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢
- ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£ ‡∏≠‡πà‡∏≤‡∏ô‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
- ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏™‡∏∞‡∏Å‡∏î‡∏ú‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Å‡∏≤‡∏£‡∏™‡πÅ‡∏Å‡∏ô (OCR)
- ‡∏´‡πâ‡∏≤‡∏°‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏´‡∏£‡∏∑‡∏≠‡∏•‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö:
{text[:5000]}
"""
        res_correct = requests.post(OLLAMA_URL, json={
            "model": OLLAMA_MODEL,
            "prompt": prompt_correct,
            "stream": False
        })
        corrected_text = res_correct.json().get("response", "").strip()

        # ===== ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà 2: ‡∏™‡∏£‡∏∏‡∏õ‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ó‡∏µ‡πà‡πÅ‡∏Å‡πâ‡πÅ‡∏•‡πâ‡∏ß =====
        prompt_summary = f"""
‡∏Ñ‡∏∏‡∏ì‡πÄ‡∏õ‡πá‡∏ô‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏£‡∏≠‡∏ö‡∏Ñ‡∏£‡∏±‡∏ß
‡πÇ‡∏õ‡∏£‡∏î‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡πÇ‡∏î‡∏¢‡πÅ‡∏ö‡πà‡∏á‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠‡πÄ‡∏õ‡πá‡∏ô:
- ‡∏Ñ‡∏π‡πà‡∏Å‡∏£‡∏ì‡∏µ
- ‡πÄ‡∏´‡∏ï‡∏∏‡∏ü‡πâ‡∏≠‡∏á
- ‡∏Ñ‡∏≥‡∏Ç‡∏≠‡∏ó‡πâ‡∏≤‡∏¢‡∏ü‡πâ‡∏≠‡∏á

‡πÉ‡∏´‡πâ‡πÉ‡∏ä‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏ó‡∏≤‡∏á‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∞‡∏ä‡∏≤‡∏ä‡∏ô‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ
‡∏´‡πâ‡∏≤‡∏°‡∏™‡∏•‡∏±‡∏ö‡∏ö‡∏ó‡∏ö‡∏≤‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏π‡πà‡∏Å‡∏£‡∏ì‡∏µ (‡πÇ‡∏à‡∏ó‡∏Å‡πå‡πÄ‡∏õ‡πá‡∏ô‡∏ù‡πà‡∏≤‡∏¢‡∏´‡∏ç‡∏¥‡∏á, ‡∏à‡∏≥‡πÄ‡∏•‡∏¢‡πÄ‡∏õ‡πá‡∏ô‡∏ù‡πà‡∏≤‡∏¢‡∏ä‡∏≤‡∏¢)
‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏™‡πà‡πÄ‡∏•‡∏Ç‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏´‡∏£‡∏∑‡∏≠‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå‡∏û‡∏¥‡πÄ‡∏®‡∏©

‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (‡∏´‡∏•‡∏±‡∏á‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç):
{corrected_text}
"""
        res_summary = requests.post(OLLAMA_URL, json={
            "model": OLLAMA_MODEL,
            "prompt": prompt_summary,
            "stream": False
        })
        summary = res_summary.json().get("response", "").strip()

        # ‡πÄ‡∏Å‡πá‡∏ö‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô session
        session["chat_context"] = summary

        return jsonify({
            "type": "summary",
            "corrected": corrected_text,
            "summary": summary,
            "message": "‚úÖ ‡πÅ‡∏Å‡πâ‡∏Ñ‡∏≥‡∏™‡∏∞‡∏Å‡∏î‡πÅ‡∏•‡∏∞‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏£‡∏µ‡∏¢‡∏ö‡∏£‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡πâ‡∏ß"
        })

    # ==== ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ====
    data = request.get_json()
    msg = data.get("message", "").strip()
    if not msg:
        return jsonify({"error": "‚ùå ‡πÇ‡∏õ‡∏£‡∏î‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°"}), 400

    # ===== ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢ =====
    rag_contexts = search_similar(msg, law_texts, law_index)
    law_context = "\n\n".join([t for t, _ in rag_contexts])

    # ===== ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á =====
    lawyer_matches = search_similar(msg, lawyer_texts, lawyer_index, top_k=3)
    related_lawyers = []
    for text, _ in lawyer_matches:
        for l in lawyer_data:
            if l["name"] in text:
                related_lawyers.append(l)

    if related_lawyers:
        session["last_lawyer"] = related_lawyers[0]

    # ===== ‡∏£‡∏ß‡∏° context =====
    user_context = session.get("chat_context", "")
    full_context = f"{user_context}\n\n{law_context}" if user_context else law_context

    lawyer_suggestions = "\n".join(
        [f"- {l['name']} ({', '.join(l['expertise'])})" for l in related_lawyers]
    )

    prompt = f"""
‡∏Ñ‡∏∏‡∏ì‡∏Ñ‡∏∑‡∏≠‡∏ú‡∏π‡πâ‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏Ç‡πâ‡∏≠‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°: {msg}

‡∏ö‡∏£‡∏¥‡∏ö‡∏ó:
{full_context}

‡∏ó‡∏ô‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á:
{lawyer_suggestions}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢ ‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏°‡∏≤‡∏ï‡∏£‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á ‡∏û‡∏£‡πâ‡∏≠‡∏°‡∏¢‡∏Å‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ)
‡πÅ‡∏•‡∏∞‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤
"‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡πà‡∏≠‡∏ó‡∏ô‡∏≤‡∏¢‡πÑ‡∏´‡∏°?"
"""
    res = requests.post(OLLAMA_URL, json={
        "model": OLLAMA_MODEL,
        "prompt": prompt,
        "stream": False
    })
    answer = res.json().get("response", "").strip()

    return jsonify({
        "type": "answer",
        "message": answer,
        "lawyers": related_lawyers
    })


# ===== MAIN =====
if __name__ == "__main__":
    os.makedirs("cache", exist_ok=True)
    os.makedirs("embeddings", exist_ok=True)

    if os.path.exists(TEXTS_LAW):
        with open(TEXTS_LAW, "r", encoding="utf-8") as f:
            law_texts = json.load(f)
        law_index = faiss.read_index(INDEX_LAW)
    else:
        law_texts, law_index = build_law_dataset()
        faiss.write_index(law_index, INDEX_LAW)
        with open(TEXTS_LAW, "w", encoding="utf-8") as f:
            json.dump(law_texts, f, ensure_ascii=False)

    if os.path.exists(TEXTS_LAWYER):
        with open(TEXTS_LAWYER, "r", encoding="utf-8") as f:
            lawyer_data = json.load(f)
        lawyer_texts = [f"{d['name']} - ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏ä‡∏µ‡πà‡∏¢‡∏ß‡∏ä‡∏≤‡∏ç: {', '.join(d['expertise'])}" for d in lawyer_data]
        lawyer_index = faiss.read_index(INDEX_LAWYER)
    else:
        lawyer_data, lawyer_texts, lawyer_index = build_lawyer_dataset()
        faiss.write_index(lawyer_index, INDEX_LAWYER)
        with open(TEXTS_LAWYER, "w", encoding="utf-8") as f:
            json.dump(lawyer_data, f, ensure_ascii=False)

    print("üéØ ‡∏£‡∏∞‡∏ö‡∏ö‡πÅ‡∏ä‡∏ó‡∏Å‡∏é‡∏´‡∏°‡∏≤‡∏¢‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡πÅ‡∏•‡πâ‡∏ß")
    app.run(host="0.0.0.0", port=8000, debug=False)
